{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wvpqmFuBTf7T",
    "outputId": "62ca8f7c-4778-4d9e-8cb9-bc965752860c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jsonlines\n",
      "  Downloading jsonlines-3.0.0-py3-none-any.whl (8.5 kB)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonlines) (3.10.0.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from jsonlines) (21.4.0)\n",
      "Installing collected packages: jsonlines\n",
      "Successfully installed jsonlines-3.0.0\n"
     ]
    }
   ],
   "source": [
    "pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YfQBZwo6NopD"
   },
   "outputs": [],
   "source": [
    "#import libraries\n",
    "\n",
    "import csv\n",
    "import jsonlines\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import SGD, Adam\n",
    "from typing import *\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Y6G_zORmTllO"
   },
   "outputs": [],
   "source": [
    "# def of paths\n",
    "\n",
    "pth_train = \"/content/drive/MyDrive/data/train.jsonl\"\n",
    "pth_dev = \"/content/drive/MyDrive/data/dev.jsonl\"\n",
    "pth_test = \"/content/drive/MyDrive/data/test.jsonl\"\n",
    "pathGlove = \"/content/drive/MyDrive/Copia di glove.6B.300d.txt\"\n",
    "path_out_dev = \"/content/predictions_dev.tsv\"\n",
    "path_out_test = \"/content/predictions_test.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ywsz7Ve-Rdug"
   },
   "outputs": [],
   "source": [
    "#organization of dataset\n",
    "\n",
    "def organize_data(path: str, labels_on=True):\n",
    "\n",
    "  data = jsonlines.open(path)\n",
    "  texts = []\n",
    "  new_texts = []\n",
    "  labels = []\n",
    "  ids = []\n",
    "\n",
    "  if(labels_on):\n",
    "\n",
    "    for line in data:\n",
    "\n",
    "      #read texts of dataset\n",
    "      text = line['text']   \n",
    "      #remove punctuation from words\n",
    "      text = ''.join( c for c in text if c not in string.punctuation) \n",
    "      #save texts, labels and ids\n",
    "      texts.append(line['text'])\n",
    "      labels.append(line['label'])\n",
    "      ids.append(line['id'])\n",
    "\n",
    "    #create a list of words from texts\n",
    "    for t in texts:\n",
    "      new_texts.append(t.split(\" \"))\n",
    "\n",
    "    #checks\n",
    "    assert len(new_texts) == len(labels) == len(ids) \n",
    "  \n",
    "    return new_texts, labels, ids\n",
    "\n",
    "  else:\n",
    "\n",
    "    for line in data:\n",
    "\n",
    "      #read texts of dataset\n",
    "      text = line['text']\n",
    "      #remove punctuation from words\n",
    "      text = ''.join( c for c in text if c not in string.punctuation) \n",
    "      #save texts and ids\n",
    "      texts.append(line['text'])\n",
    "      ids.append(line['id'])\n",
    "\n",
    "    #create a list of words from texts\n",
    "    for t in texts:\n",
    "      new_texts.append(t.split(\" \")) \n",
    "\n",
    "    #checks\n",
    "    assert len(new_texts) == len(ids) \n",
    "\n",
    "    return new_texts, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "SL0Ds4S4ULxO"
   },
   "outputs": [],
   "source": [
    "#transformation of words in tensors using previos vocabulary\n",
    "\n",
    "def VecFromWord(voc: dict, text: list):\n",
    "\n",
    "  new_vecs = []\n",
    "\n",
    "  for t in text:\n",
    "\n",
    "    #associate each word in every text to a tensor, using the vocabulary(pre-trained word-embedding)\n",
    "    vecs = transform(voc, t)\n",
    "\n",
    "    # form: [[tensor1_1,....,tensor1_N],...,[tensorM_1,.....,tensorM_N]]\n",
    "    new_vecs.append(vecs)\n",
    "\n",
    "  print(\"Found \"+str(len(new_vecs))+\" samples.\")\n",
    "\n",
    "  return new_vecs\n",
    "\n",
    "#function for effective transormation\n",
    "def transform(v: dict, l: list):\n",
    "\n",
    "  vec = []\n",
    "\n",
    "  for word in l:\n",
    "    word = word.lower()\n",
    "    if word in v.keys():\n",
    "      vec.append(v[word].squeeze(0))\n",
    "    #Oss: ingoring missing words from vocabulary\n",
    "\n",
    "  return vec\n",
    "\n",
    "#function to do the mean between the words of a text\n",
    "def aggregation(data: list):\n",
    "\n",
    "  new_data = []\n",
    "\n",
    "  for d in data:\n",
    "\n",
    "    sum_vec = []\n",
    "\n",
    "    summ = 0\n",
    "    for el in d:\n",
    "      summ+=el\n",
    "    summ = summ/len(d)\n",
    "\n",
    "    new_data.append(summ)\n",
    "\n",
    "  #print(\"Found \"+str(len(new_data))+\" samples.\")\n",
    "\n",
    "  return torch.stack(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t865MZCfIV8F"
   },
   "outputs": [],
   "source": [
    "#Find the class belonging to data and encode in one-hot encoding\n",
    "\n",
    "def define_classes(labels: list):\n",
    "\n",
    "  #find number of different classes\n",
    "  classes = []\n",
    "  for i in np.unique(labels):\n",
    "    classes.append(i)\n",
    "  num_class = len(classes)\n",
    "\n",
    "  #associate an integer to each class\n",
    "  classes_dict = {}\n",
    "  counter = 0\n",
    "  for elem in classes:\n",
    "    if elem not in classes_dict.keys():\n",
    "        classes_dict[elem]=counter\n",
    "        counter+=1\n",
    "\n",
    "  print(\"Found \"+str(num_class)+\" classes\")\n",
    "\n",
    "  #create a dictionary where each int correspond to a classe encoded with one-hot encoding\n",
    "  ref = list(classes_dict.values())\n",
    "  app = torch.tensor(ref)\n",
    "  new_labels = list(F.one_hot(app, num_classes=num_class))\n",
    "  one_hot_dict = {}\n",
    "  for i,j in zip(ref,new_labels):\n",
    "    one_hot_dict.update({i:j})\n",
    "\n",
    "  #create a set inf form: [[tenso1_1,...tensor1_N],...,[tensorM_1,...tensorM_N]]\n",
    "  #where each list of tensors is a class represented in one-hot encoding\n",
    "  y = []\n",
    "  for i in labels:\n",
    "    y.append(one_hot_dict.get(classes_dict.get(i)))\n",
    "  y = torch.stack(y).squeeze()\n",
    "\n",
    "  return y, classes_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ThCipfMSU0-Q"
   },
   "outputs": [],
   "source": [
    "#create a dataset linking each text to its class, useful to pass data at the classification model\n",
    "#Oss: this function has a double scope, it can link text to its ids to create the dataset for test (where there are not labels)\n",
    "\n",
    "def create_dataset(x,y,batch_size):\n",
    "  dataset = []\n",
    "  dataset = [(data,label) for data,label in zip(x,y)]\n",
    "  return DataLoader(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "kUOcPHthj8xg"
   },
   "outputs": [],
   "source": [
    "#create a vocabulary that link each word to a set of tensor, using a pre-trained word embedding\n",
    "\n",
    "def vocabulary(pth):\n",
    "\n",
    "  #read pre-trained glove model\n",
    "  with open(pth) as f:\n",
    "    glove = f.readlines()\n",
    "  glove = [i.split(maxsplit = 1) for i in glove]\n",
    "\n",
    "  #create a vocabulary from previous pre-trained model\n",
    "  vocabulary = {}\n",
    "  tensor = torch.tensor\n",
    "  for w, vec in glove:\n",
    "    vec = vec[:-2] #cut '\\n'\n",
    "    vec2 = vec.split()\n",
    "    #i = 0\n",
    "    #vec3 = [len(vec2)]\n",
    "    vec3 = [np.float64(vec2[i]) for i in range(len(vec2))]\n",
    "    tensor = torch.tensor(vec3)\n",
    "    vocabulary.update({w:tensor})\n",
    "\n",
    "  return vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "NINznnj3Seln"
   },
   "outputs": [],
   "source": [
    "#fuction to invert keys and values of a dictionary\n",
    "def invert_dictionary(classes: dict):\n",
    "\n",
    "  items_list = classes.items()\n",
    "  new_dict = {}\n",
    "  for i in list(items_list):\n",
    "    new_dict.update({i[1]:i[0]})\n",
    "\n",
    "  return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "maYa6L22_S8y"
   },
   "outputs": [],
   "source": [
    "#classification model\n",
    "\n",
    "class SentenceClassifier(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, n_features: int, n_hidden: int, n_classes: int, loss):\n",
    "        super().__init__()\n",
    "        self.layerIn = torch.nn.Linear(n_features, n_hidden)\n",
    "        self.hidden1 = torch.nn.Linear(n_hidden, 100)\n",
    "        self.hidden2 = torch.nn.Linear(100,100)\n",
    "        self.hidden3 = torch.nn.Linear(100,80)\n",
    "        self.hidden4 = torch.nn.Linear(80,70)\n",
    "        self.hidden5 = torch.nn.Linear(70,50)\n",
    "        self.hidden6 = torch.nn.Linear(50,30)\n",
    "        self.hidden7 = torch.nn.Linear(30,20)\n",
    "        #self.hidden8 = torch.nn.Linear(50,50)\n",
    "        #self.hidden9 = torch.nn.Linear(50,50)\n",
    "        #self.hidden10 = torch.nn.Linear(50,30)\n",
    "        self.layerOut = torch.nn.Linear(20,n_classes)\n",
    "        self.loss_fn = loss\n",
    "\n",
    "    def forward(self, x: torch.Tensor, y: Optional[torch.Tensor] = None) -> Dict[str, torch.Tensor]:\n",
    "        \n",
    "        out = self.layerIn(x)\n",
    "        out = torch.relu(out)\n",
    "        out = self.hidden1(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.hidden2(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.hidden3(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.hidden4(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.hidden5(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.hidden6(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.hidden7(out)\n",
    "        out = torch.relu(out)\n",
    "        '''out = self.hidden8(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.hidden9(out)\n",
    "        out = torch.relu(out)\n",
    "        out = self.hidden10(out)\n",
    "        out = torch.relu(out)'''\n",
    "        out = self.layerOut(out)\n",
    "        out = nn.Softmax(dim=1)(out)\n",
    "\n",
    "        result = {'pred': out}\n",
    "\n",
    "        if y is not None:\n",
    "            loss = self.loss_fn(out, y)\n",
    "            result['loss'] = loss\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LBeo38uY_WMd"
   },
   "outputs": [],
   "source": [
    "def train(model: nn.Module, optimizer: torch.optim.Optimizer, data: DataLoader, epochs: int = 20):\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(\"\\nepoch: \"+str(epoch+1)+\"/\"+str(epochs))\n",
    "        tot_loss = []\n",
    "\n",
    "        #batches of the training set\n",
    "        for x, y in data:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            data_out = model(x.type(torch.float), y.type(torch.float))\n",
    "            loss = data_out['loss']\n",
    "\n",
    "            tot_loss.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(\"loss: \"+str(sum(tot_loss)/len(tot_loss))+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7BTSauCP_Yq6"
   },
   "outputs": [],
   "source": [
    "def test(model: nn.Module, dataloader: DataLoader):\n",
    "\n",
    "  num_true = 0\n",
    "  dim_set = 0\n",
    "\n",
    "  for x, y in dataloader:\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        data_out = model(x.type(torch.float))\n",
    "        pred = data_out['pred']\n",
    "\n",
    "    for i,j in zip(pred,y):\n",
    "        p = torch.argmax(i)\n",
    "        truth = torch.argmax(j)\n",
    "        dim_set += 1\n",
    "        num_true += (truth == p).int()\n",
    "\n",
    "  acc = ((num_true / dim_set).item())*100\n",
    "  err = (1 - (num_true / dim_set).item())*100\n",
    "\n",
    "  print(f'# accuracy: {acc:.2f}')\n",
    "  print(f'# error-rate: {err:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "TULe6qJ1I1f9"
   },
   "outputs": [],
   "source": [
    "def predictions(model: nn.Module, dataloader: DataLoader, classes: dict, pth_out: str):\n",
    "\n",
    "  test_pred = []\n",
    "  \n",
    "  for x, y in dataloader:\n",
    "            \n",
    "    with torch.no_grad():\n",
    "        data_out = model(x.type(torch.float))\n",
    "        pred = data_out['pred']\n",
    "\n",
    "    for i,j in zip(pred,y):\n",
    "        p = torch.argmax(i).item()\n",
    "        c = str(j.item())+'\\t'+classes.get(p)\n",
    "        test_pred.append(c)\n",
    "\n",
    "  #open a .tsv file and save data on it\n",
    "  with open(pth_out, 'w') as f_output:\n",
    "    tsv_output = csv.writer(f_output, delimiter='\\n',quotechar =' ')\n",
    "    tsv_output.writerow(test_pred)\n",
    "\n",
    "  print(\"File saved.\")\n",
    "\n",
    "  return test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "m7jCYDDuMghL"
   },
   "outputs": [],
   "source": [
    "#HYPERPARAMETERS\n",
    "\n",
    "EPOCHS = 20\n",
    "opt = Adam\n",
    "lr=0.001\n",
    "batches = 200\n",
    "loss = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "2li7pCZEkzLU"
   },
   "outputs": [],
   "source": [
    "#read the pre-trained word embedding\n",
    "vocab = vocabulary(pathGlove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kxy_Dtk4zYhh",
    "outputId": "c27c00f2-4ab1-4985-f804-1b2ed137bb19"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 186282 samples.\n",
      "Found 15 classes\n"
     ]
    }
   ],
   "source": [
    "#read training data\n",
    "\n",
    "texts_t, labels_t, ids_t = organize_data(pth_train)\n",
    "list_of_tensors_t = VecFromWord(vocab, texts_t)\n",
    "x_train = aggregation(list_of_tensors_t)\n",
    "y_train, _ = define_classes(labels_t)\n",
    "train_data = create_dataset(x_train,y_train,batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sIQ9G_YhVxIs",
    "outputId": "5ed7dd7a-c4c4-4677-f36f-4b42993eebde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6844 samples.\n",
      "Found 15 classes\n"
     ]
    }
   ],
   "source": [
    "#read dev data\n",
    "\n",
    "texts_d, labels_d, ids_d = organize_data(pth_dev)\n",
    "list_of_tensors_d = VecFromWord(vocab, texts_d)\n",
    "x_dev = aggregation(list_of_tensors_d)\n",
    "y_dev, classes_dict = define_classes(labels_d)\n",
    "dev_data = create_dataset(x_dev,y_dev,batches) #set for test the model\n",
    "dev_data_for_test = create_dataset(x_dev, ids_d, batches) #set for generate tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SJ8cFapgMeuu",
    "outputId": "1226849e-2025-43e3-dfaa-b01191334847"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 6849 samples.\n"
     ]
    }
   ],
   "source": [
    "#read dev data\n",
    "\n",
    "texts_test, ids_test = organize_data(pth_test, labels_on=False)\n",
    "list_of_tensors_test = VecFromWord(vocab, texts_test)\n",
    "x_test = aggregation(list_of_tensors_test)\n",
    "test_data = create_dataset(x_test, ids_test,batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "mBWzue39AeLP"
   },
   "outputs": [],
   "source": [
    "#define model and opitmizer\n",
    "n_feature = x_train.shape[1]\n",
    "n_class = y_train.shape[1]\n",
    "model = SentenceClassifier(n_feature, n_feature, n_class, loss)\n",
    "optimizer = opt(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P-Byo7UGBEH6",
    "outputId": "dcede800-3868-45b7-a6ef-5d326e3a5975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1/20\n",
      "loss: 0.03458220688746083\n",
      "\n",
      "\n",
      "epoch: 2/20\n",
      "loss: 0.0249467492313424\n",
      "\n",
      "\n",
      "epoch: 3/20\n",
      "loss: 0.022667509973320968\n",
      "\n",
      "\n",
      "epoch: 4/20\n",
      "loss: 0.02166410177734245\n",
      "\n",
      "\n",
      "epoch: 5/20\n",
      "loss: 0.020976291498760874\n",
      "\n",
      "\n",
      "epoch: 6/20\n",
      "loss: 0.020435712552057828\n",
      "\n",
      "\n",
      "epoch: 7/20\n",
      "loss: 0.01999594488924633\n",
      "\n",
      "\n",
      "epoch: 8/20\n",
      "loss: 0.019581406411231204\n",
      "\n",
      "\n",
      "epoch: 9/20\n",
      "loss: 0.019175931418618355\n",
      "\n",
      "\n",
      "epoch: 10/20\n",
      "loss: 0.018796366590470907\n",
      "\n",
      "\n",
      "epoch: 11/20\n",
      "loss: 0.01840076168055762\n",
      "\n",
      "\n",
      "epoch: 12/20\n",
      "loss: 0.018141225538947998\n",
      "\n",
      "\n",
      "epoch: 13/20\n",
      "loss: 0.017877186053980126\n",
      "\n",
      "\n",
      "epoch: 14/20\n",
      "loss: 0.017571812613503487\n",
      "\n",
      "\n",
      "epoch: 15/20\n",
      "loss: 0.017394943033892645\n",
      "\n",
      "\n",
      "epoch: 16/20\n",
      "loss: 0.017183728108398393\n",
      "\n",
      "\n",
      "epoch: 17/20\n",
      "loss: 0.01700854342822545\n",
      "\n",
      "\n",
      "epoch: 18/20\n",
      "loss: 0.01679049124301695\n",
      "\n",
      "\n",
      "epoch: 19/20\n",
      "loss: 0.0166245567894145\n",
      "\n",
      "\n",
      "epoch: 20/20\n",
      "loss: 0.016461902766692346\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "train(model,optimizer,train_data,EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BWDvYHYKP0ES",
    "outputId": "e19e4674-7e37-4f86-cba3-1e47100f0b66"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# accuracy: 75.28\n",
      "# error-rate: 24.72\n"
     ]
    }
   ],
   "source": [
    "#test the model\n",
    "test(model, dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EgbxHINzM7U0",
    "outputId": "29b4a772-3f0b-4d80-9bc2-c7652ac10a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File saved.\n",
      "File saved.\n"
     ]
    }
   ],
   "source": [
    "#predict and save predictions into a .tsv file\n",
    "new_class_dict = invert_dictionary(classes_dict)\n",
    "pred_dev = predictions(model, dev_data_for_test, new_class_dict, path_out_dev)\n",
    "pred_test = predictions(model, test_data, new_class_dict, path_out_test)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "nlp_2022-bonus_exercise_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
